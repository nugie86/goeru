{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzk7iX_CodX6",
    "tags": []
   },
   "source": [
    "# GOERU - RECSYS MACHINE LEARNING\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "- [ 2 - Movie ratings dataset ](#2)\n",
    "- [ 3 - Content-based filtering with a neural network](#3)\n",
    "  - [ 3.1 Training Data](#3.1)\n",
    "  - [ 3.2 Preparing the training data](#3.2)\n",
    "- [ 4 - Neural Network for content-based filtering](#4)\n",
    "  - [ Exercise 1](#ex01)\n",
    "- [ 5 - Predictions](#5)\n",
    "  - [ 5.1 - Predictions for a new user](#5.1)\n",
    "  - [ 5.2 - Predictions for an existing user.](#5.2)\n",
    "  - [ 5.3 - Finding Similar Items](#5.3)\n",
    "    - [ Exercise 2](#ex02)\n",
    "- [ 6 - Congratulations! ](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**NOTE:** To prevent errors from the autograder, you are not allowed to edit or delete non-graded cells in this lab. Please also refrain from adding any new cells. \n",
    "**Once you have passed this assignment** and want to experiment with any of the non-graded code, you may follow the instructions at the bottom of this notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages <img align=\"left\" src=\"./images/movie_camera.png\"     style=\" width:40px;  \">\n",
    "We will use familiar packages, NumPy, TensorFlow and helpful routines from [scikit-learn](https://scikit-learn.org/stable/). We will also use [tabulate](https://pypi.org/project/tabulate/) to neatly print tables and [Pandas](https://pandas.pydata.org/) to organize tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Xu-w_RmNwCV5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "from recsysNN_utils import *\n",
    "pd.set_option(\"display.precision\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Content-based filtering with a neural network\n",
    "\n",
    "In the collaborative filtering lab, you generated two vectors, a user vector and an item/movie vector whose dot product would predict a rating. The vectors were derived solely from the ratings.   \n",
    "\n",
    "Content-based filtering also generates a user and movie feature vector but recognizes there may be other information available about the user and/or movie that may improve the prediction. The additional information is provided to a neural network which then generates the user and movie vector as shown below.\n",
    "<figure>\n",
    "    <center> <img src=\"./images/RecSysNN.png\"   style=\"width:500px;height:280px;\" ></center>\n",
    "</figure>\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Training Data\n",
    "The movie content provided to the network is a combination of the original data and some 'engineered features'. Recall the feature engineering discussion and lab from Course 1, Week 2, lab 4. The original features are the year the movie was released and the movie's genre's presented as a one-hot vector. There are 14 genres. The engineered feature is an average rating derived from the user ratings. \n",
    "\n",
    "The user content is composed of engineered features. A per genre average rating is computed per user. Additionally, a user id, rating count and rating average are available but not included in the training or prediction content. They are carried with the data set because they are useful in interpreting data.\n",
    "\n",
    "The training set consists of all the ratings made by the users in the data set. Some ratings are repeated to boost the number of training examples of underrepresented genre's. The training set is split into two arrays with the same number of entries, a user array and a movie/item array.  \n",
    "\n",
    "Below, let's load and display some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "M5gfMLYgxCD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training vectors: 15000\n"
     ]
    }
   ],
   "source": [
    "# Load Data, set configuration variables\n",
    "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict = load_data()\n",
    "\n",
    "num_user_features = user_train.shape[1] - 2  # remove userid, rating count and ave rating during training\n",
    "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
    "uvs = 2  # user genre vector start\n",
    "ivs = 2  # item genre vector start\n",
    "u_s = 2  # start of columns to use in training, user\n",
    "i_s = 1  # start of columns to use in training, items\n",
    "print(f\"Number of training vectors: {len(item_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few entries in the user training array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [use r_id] </th><th style=\"text-align: center;\"> [age] </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [use r_id] </th><th style=\"text-align: center;\"> [age] </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">  22   </td><td style=\"text-align: center;\">     0.0      </td><td style=\"text-align: center;\">   4.1    </td><td style=\"text-align: center;\">   3.7    </td><td style=\"text-align: center;\">    3.6     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">    0.0     </td><td style=\"text-align: center;\">   0.0    </td><td style=\"text-align: center;\">     0.0      </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the user and item/movie features are not used in training. In the table above, the features in brackets \"[]\" such as the \"user id\", \"rating count\" and \"rating ave\" are not included when the model is trained and used.\n",
    "Above you can see the per genre rating average for user 2. Zero entries are genre's which the user had not rated. The user vector is the same for all the movies rated by a user.  \n",
    "Let's look at the first few entries of the movie/item array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [teach er_id] </th><th style=\"text-align: center;\"> ave_r atings </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\n",
       "<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\n",
       "<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\n",
       "<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\n",
       "<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [teach er_id] </th><th style=\"text-align: center;\"> ave_r atings </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\\n<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\\n<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\\n<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\\n<tr><td style=\"text-align: center;\">       1       </td><td style=\"text-align: center;\">     3.67     </td><td style=\"text-align: center;\">      1       </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">    0     </td><td style=\"text-align: center;\">      0       </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(item_train, item_features, ivs, i_s, user=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the movie array contains the year the film was released, the average rating and an indicator for each potential genre. The indicator is one for each genre that applies to the movie. The movie id is not used in training but is useful when interpreting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[:5]: [4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train[:5]: {y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target, y, is the movie rating given by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that movie 6874 is an Action/Crime/Thriller movie released in 2003. User 2 rates action movies as 3.9 on average. MovieLens users gave the movie an average rating of 4. 'y' is 4 indicating user 2 rated movie 6874 as a 4 as well. A single training example consists of a row from both the user and item arrays and a rating from y_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Preparing the training data\n",
    "Recall in Course 1, Week 2, you explored feature scaling as a means of improving convergence. We'll scale the input features using the [scikit learn StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). This was used in Course 1, Week 2, Lab 5.  Below, the inverse_transform is also shown to produce the original inputs. We'll scale the target ratings using a Min Max Scaler which scales the target to be between -1 and 1. [scikit learn MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# scale training data\n",
    "item_train_unscaled = item_train\n",
    "user_train_unscaled = user_train\n",
    "y_train_unscaled    = y_train\n",
    "\n",
    "scalerItem = StandardScaler()\n",
    "scalerItem.fit(item_train)\n",
    "item_train = scalerItem.transform(item_train)\n",
    "\n",
    "scalerUser = StandardScaler()\n",
    "scalerUser.fit(user_train)\n",
    "user_train = scalerUser.transform(user_train)\n",
    "\n",
    "scalerTarget = MinMaxScaler((-1, 1))\n",
    "scalerTarget.fit(y_train.reshape(-1, 1))\n",
    "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
    "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow us to evaluate the results, we will split the data into training and test sets as was discussed in Course 2, Week 3. Here we will use [sklean train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split and shuffle the data. Note that setting the initial random state to the same value ensures item, user, and y are shuffled identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item training data shape: (12000, 11)\n",
      "item test data shape: (3000, 11)\n"
     ]
    }
   ],
   "source": [
    "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True)\n",
    "print(f\"item training data shape: {item_train.shape}\")\n",
    "print(f\"item test data shape: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled, shuffled data now has a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [use r_id] </th><th style=\"text-align: center;\"> [age] </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">   0   </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.3    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   -0.8   </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     -0.8     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    1.4     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     1.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    1.4     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     1.0      </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">   0   </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.3    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   -0.8   </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     -0.8     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.0    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">     1.4      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [use r_id] </th><th style=\"text-align: center;\"> [age] </th><th style=\"text-align: center;\"> Mathe matics </th><th style=\"text-align: center;\"> Phy sics </th><th style=\"text-align: center;\"> Bio logy </th><th style=\"text-align: center;\"> Chem istry </th><th style=\"text-align: center;\"> Eco nomy </th><th style=\"text-align: center;\"> Sosi ology </th><th style=\"text-align: center;\"> Geog raphy </th><th style=\"text-align: center;\"> His tory </th><th style=\"text-align: center;\"> Antro pology </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">   0   </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.3    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   -0.8   </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     -0.8     </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    1.4     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     1.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    1.4     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     1.0      </td></tr>\\n<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">   0   </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.3    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   -0.8   </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">     -0.8     </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">   1   </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">    -0.7    </td><td style=\"text-align: center;\">   1.0    </td><td style=\"text-align: center;\">    -0.3    </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">     1.4      </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Neural Network for content-based filtering\n",
    "Now, let's construct a neural network as described in the figure above. It will have two networks that are combined by a dot product. You will construct the two networks. In this example, they will be identical. Note that these networks do not need to be the same. If the user content was substantially larger than the movie content, you might elect to increase the complexity of the user network relative to the movie network. In this case, the content is similar, so the networks are the same.\n",
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "- Use a Keras sequential model\n",
    "    - The first layer is a dense layer with 256 units and a relu activation.\n",
    "    - The second layer is a dense layer with 128 units and a relu activation.\n",
    "    - The third layer is a dense layer with `num_outputs` units and a linear or no activation.   \n",
    "    \n",
    "The remainder of the network will be provided. The provided code does not use the Keras sequential model but instead uses the Keras [functional api](https://keras.io/guides/functional_api/). This format allows for more flexibility in how components are interconnected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "id": "CBjZ2HhRwpa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32)           39584       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 32)           39840       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize (TFOpLamb  (None, 32)          0           ['sequential[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_1 (TFOpLa  (None, 32)          0           ['sequential_1[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['tf.math.l2_normalize[0][0]',   \n",
      "                                                                  'tf.math.l2_normalize_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 79,424\n",
      "Trainable params: 79,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRADED_CELL\n",
    "# UNQ_C1\n",
    "\n",
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "  \n",
    "  \n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "  \n",
    "  \n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a mean squared error loss and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pGK5MEUowxN4"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6zHf7eASw0tN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 4s 6ms/step - loss: 0.1888\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1862\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1855\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1848\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1845\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1842\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1840\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1839\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1835\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1835\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1831\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1830\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1828\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.1826\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.1826\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.1824\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1822\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1821\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.1820\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1819\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1816\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1816\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1814\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1813\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1812\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1811\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1810\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1809\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1806\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1807\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1807\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1800\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1804\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1802\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1803\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1802\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1800\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1797\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1797\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1799\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1796\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1797\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1794\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1793\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1793\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1792\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1791\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.1792\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1788\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1787\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1790\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1788\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1784\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1784\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1783\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.1781\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1781\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1780\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1777\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1777\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1775\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1774\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1780\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1772\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1772\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1772\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1769\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1770\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1769\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1769\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1767\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1765\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1766\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1768\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1763\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1762\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1763\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1761\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1768\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1761\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1756\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1757\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1757\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1757\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1751\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1748\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1746\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1746\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1746\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1744\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1743\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1744\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1742\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1737\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1739\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1736\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1733\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1734\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1737\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c43d8d6a30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to determine loss on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1s 3ms/step - loss: 0.2027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2027108520269394"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is comparable to the training loss indicating the model has not substantially overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsre-gquwEls"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Predictions\n",
    "Below, you'll use your model to make predictions in a number of circumstances. \n",
    "<a name=\"5.1\"></a>\n",
    "### 5.1 - Predictions for a new user\n",
    "First, we'll create a new user and have the model suggest movies for that user. After you have tried this on the example user content, feel free to change the user content to match your own preferences and see what the model suggests. Note that ratings are between 0.5 and 5.0, inclusive, in half-step increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "id": "4_7nZyPiVJ4r"
   },
   "outputs": [],
   "source": [
    "new_user_id = 5000\n",
    "new_age = 20\n",
    "new_Mathematics = 5.0\n",
    "new_Physics = 5.0\n",
    "new_Biology = 0.0\n",
    "new_Chemistry = 0.0\n",
    "new_Economy = 0.0\n",
    "new_Sosiology = 0.0\n",
    "new_Geography = 0.0\n",
    "new_History = 0.0\n",
    "new_Antropology = 0.0\n",
    "\n",
    "user_vec = np.array([[new_user_id, new_age, new_Mathematics,\n",
    "                      new_Physics, new_Biology, new_Chemistry,\n",
    "                      new_Economy,new_Sosiology, new_Geography,\n",
    "                      new_History,new_Antropology\n",
    "                      ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new user enjoys movies from the adventure, fantasy genres. Let's find the top-rated movies for the new user.  \n",
    "Below, we'll use a set of movie/item vectors, `item_vecs` that have a vector for each movie in the training/test set. This is matched with the new user vector above and the scaled vectors are used to predict ratings for all the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  teacher id</th><th style=\"text-align: right;\">  rating ave</th><th>name                   </th><th>fields                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">  4.7</td><td style=\"text-align: right;\">          57</td><td style=\"text-align: right;\">         4.7</td><td>Kevin Rafli Basalamah  </td><td>Mathematics | Chemistry |                                </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.7</td><td style=\"text-align: right;\">         155</td><td style=\"text-align: right;\">         4.2</td><td>Hanif Arshaka Arighi   </td><td>Economy | History |                                      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         139</td><td style=\"text-align: right;\">         4.1</td><td>Mahila Fauzia Rahmana  </td><td>Economy | Geography | History | Antropologi |            </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         197</td><td style=\"text-align: right;\">         4.1</td><td>Calila Nisya Syifa     </td><td>Economy | Sosiology | Geography | History | Antropologi |</td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         163</td><td style=\"text-align: right;\">         4.1</td><td>Hanif Rashid Arighi    </td><td>Economy | Sosiology | Geography | History | Antropologi |</td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         176</td><td style=\"text-align: right;\">         4.9</td><td>Nayla Maheswari Rahmana</td><td>Economy | Geography |                                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         124</td><td style=\"text-align: right;\">         4.6</td><td>Sabrina Putri Syifa    </td><td>Antropologi |                                            </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         192</td><td style=\"text-align: right;\">         4.4</td><td>Sabrina Putri Salsabila</td><td>Sosiology | Geography | Antropologi |                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">          73</td><td style=\"text-align: right;\">         4.1</td><td>Zulfan Arshaka Khattab </td><td>Mathematics | Physics | Chemistry |                      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">         4.2</td><td>Hikam Fahrian Khattab  </td><td>Mathematics | Physics | Chemistry |                      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  teacher id</th><th style=\"text-align: right;\">  rating ave</th><th>name                   </th><th>fields                                                   </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: right;\">  4.7</td><td style=\"text-align: right;\">          57</td><td style=\"text-align: right;\">         4.7</td><td>Kevin Rafli Basalamah  </td><td>Mathematics | Chemistry |                                </td></tr>\\n<tr><td style=\"text-align: right;\">  4.7</td><td style=\"text-align: right;\">         155</td><td style=\"text-align: right;\">         4.2</td><td>Hanif Arshaka Arighi   </td><td>Economy | History |                                      </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         139</td><td style=\"text-align: right;\">         4.1</td><td>Mahila Fauzia Rahmana  </td><td>Economy | Geography | History | Antropologi |            </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         197</td><td style=\"text-align: right;\">         4.1</td><td>Calila Nisya Syifa     </td><td>Economy | Sosiology | Geography | History | Antropologi |</td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         163</td><td style=\"text-align: right;\">         4.1</td><td>Hanif Rashid Arighi    </td><td>Economy | Sosiology | Geography | History | Antropologi |</td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         176</td><td style=\"text-align: right;\">         4.9</td><td>Nayla Maheswari Rahmana</td><td>Economy | Geography |                                    </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         124</td><td style=\"text-align: right;\">         4.6</td><td>Sabrina Putri Syifa    </td><td>Antropologi |                                            </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         192</td><td style=\"text-align: right;\">         4.4</td><td>Sabrina Putri Salsabila</td><td>Sosiology | Geography | Antropologi |                    </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">          73</td><td style=\"text-align: right;\">         4.1</td><td>Zulfan Arshaka Khattab </td><td>Mathematics | Physics | Chemistry |                      </td></tr>\\n<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">         4.2</td><td>Hikam Fahrian Khattab  </td><td>Mathematics | Physics | Chemistry |                      </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate and replicate the user vector to match the number movies in the data set.\n",
    "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
    "\n",
    "# scale our user and item vectors\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# make a prediction\n",
    "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# unscale y prediction \n",
    "y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# sort the results, highest prediction first\n",
    "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "sorted_ypu   = y_pu[sorted_index]\n",
    "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
    "\n",
    "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### 5.2 - Predictions for an existing user.\n",
    "Let's look at the predictions for \"user 2\", one of the users in the data set. We can compare the predicted ratings with the model's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_to_genre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m uid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# form a set of user vectors. This is the same vector, transformed and repeated.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m user_vecs, y_vecs \u001b[38;5;241m=\u001b[39m get_user_vecs(uid, user_train_unscaled, item_vecs, \u001b[43muser_to_genre\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# scale our user and item vectors\u001b[39;00m\n\u001b[0;32m      6\u001b[0m suser_vecs \u001b[38;5;241m=\u001b[39m scalerUser\u001b[38;5;241m.\u001b[39mtransform(user_vecs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_to_genre' is not defined"
     ]
    }
   ],
   "source": [
    "uid = 2 \n",
    "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
    "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
    "\n",
    "# scale our user and item vectors\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# make a prediction\n",
    "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# unscale y prediction \n",
    "y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# sort the results, highest prediction first\n",
    "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "sorted_ypu   = y_pu[sorted_index]\n",
    "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
    "sorted_user  = user_vecs[sorted_index]\n",
    "sorted_y     = y_vecs[sorted_index]\n",
    "\n",
    "#print sorted predictions for movies rated by the user\n",
    "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction is generally within 1 of the actual rating though it is not a very accurate predictor of how a user rates specific movies. This is especially true if the user rating is significantly different than the user's genre average. You can vary the user id above to try different users. Not all user id's were used in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.3\"></a>\n",
    "### 5.3 - Finding Similar Items\n",
    "The neural network above produces two feature vectors, a user feature vector $v_u$, and a movie feature vector, $v_m$. These are 32 entry vectors whose values are difficult to interpret. However, similar items will have similar vectors. This information can be used to make recommendations. For example, if a user has rated \"Toy Story 3\" highly, one could recommend similar movies by selecting movies with similar movie feature vectors.\n",
    "\n",
    "A similarity measure is the squared distance between the two vectors $ \\mathbf{v_m^{(k)}}$ and $\\mathbf{v_m^{(i)}}$ :\n",
    "$$\\left\\Vert \\mathbf{v_m^{(k)}} - \\mathbf{v_m^{(i)}}  \\right\\Vert^2 = \\sum_{l=1}^{n}(v_{m_l}^{(k)} - v_{m_l}^{(i)})^2\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### Exercise 2\n",
    "\n",
    "Write a function to compute the square distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# GRADED_FUNCTION: sq_dist\n",
    "# UNQ_C2\n",
    "def sq_dist(a,b):\n",
    "    \"\"\"\n",
    "    Returns the squared distance between two vectors\n",
    "    Args:\n",
    "      a (ndarray (n,)): vector with n features\n",
    "      b (ndarray (n,)): vector with n features\n",
    "    Returns:\n",
    "      d (float) : distance\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###     \n",
    "    d = np.sum((a-b)**2)\n",
    "    ### END CODE HERE ###     \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
    "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
    "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
    "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
    "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
    "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "squared distance between a1 and b1: 0.000    \n",
    "squared distance between a2 and b2: 0.030   \n",
    "squared distance between a3 and b3: 2.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Public tests\n",
    "test_sq_dist(sq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "  While a summation is often an indication a for loop should be used, here the subtraction can be element-wise in one statement. Further, you can utilized np.square to square, element-wise, the result of the subtraction. np.sum can be used to sum the squared elements.\n",
    "    \n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of distances between movies can be computed once when the model is trained and then reused for new recommendations without retraining. The first step, once a model is trained, is to obtain the movie feature vector, $v_m$, for each of the movies. To do this, we will use the trained `item_NN` and build a small model to allow us to run the movie vectors through it to generate $v_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
    "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
    "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
    "model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
    "model_m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a movie model, you can create a set of movie feature vectors by using the model to predict using a set of item/movie vectors as input. `item_vecs` is a set of all of the movie vectors. It must be scaled to use with the trained model. The result of the prediction is a 32 entry feature vector for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
    "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
    "print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute a matrix of the squared distance between each movie feature vector and all other movie feature vectors:\n",
    "<figure>\n",
    "    <left> <img src=\"./images/distmatrix.PNG\"   style=\"width:400px;height:225px;\" ></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the closest movie by finding the minimum along each row. We will make use of [numpy masked arrays](https://numpy.org/doc/1.21/user/tutorial-ma.html) to avoid selecting the same movie. The masked values along the diagonal won't be included in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "count = 10  # number of movies to display\n",
    "dim = len(vms)\n",
    "dist = np.zeros((dim,dim))\n",
    "\n",
    "for i in range(dim):\n",
    "    for j in range(dim):\n",
    "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
    "        \n",
    "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
    "\n",
    "disp = [[\"name1\", \"fields1\", \"name2\", \"fields2\"]]\n",
    "for i in range(count):\n",
    "    min_idx = np.argmin(m_dist[i])\n",
    "    movie1_id = int(item_vecs[i+158,0])\n",
    "    movie2_id = int(item_vecs[min_idx,0])\n",
    "    disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
    "                  movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
    "               )\n",
    "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show the model will generally suggest a movie with similar genre's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Congratulations! <img align=\"left\" src=\"./images/film_award.png\" style=\" width:40px;\">\n",
    "You have completed a content-based recommender system.    \n",
    "\n",
    "This structure is the basis of many commercial recommender systems. The user content can be greatly expanded to incorporate more information about the user if it is available.  Items are not limited to movies. This can be used to recommend any item, books, cars or items that are similar to an item in your 'shopping cart'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Please click here if you want to experiment with any of the non-graded code.</b></font></summary>\n",
    "    <p><i><b>Important Note: Please only do this when you've already passed the assignment to avoid problems with the autograder.</b></i>\n",
    "    <ol>\n",
    "        <li> On the notebooks menu, click View > Cell Toolbar > Edit Metadata</li>\n",
    "        <li> Hit the Edit Metadata button next to the code cell which you want to lock/unlock</li>\n",
    "        <li> Set the attribute value for editable to:\n",
    "            <ul>\n",
    "                <li> true if you want to unlock it </li>\n",
    "                <li> false if you want to lock it </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> On the notebooks menu, click View > Cell Toolbar > None </li>\n",
    "    </ol>\n",
    "    <p> Here's a short demo of how to do the steps above: \n",
    "        <br>\n",
    "        <img src=\"https://drive.google.com/uc?export=view&id=14Xy_Mb17CZVgzVAgq7NCjMVBvSae3xO1\" align=\"center\" alt=\"unlock_cells.gif\">\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFYdA6zQJ1FpgYwYmRIeXa",
   "collapsed_sections": [],
   "name": "Recsys_NN.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1RO0HLb7kRE0Tj_0D4E5I-vQz2QLu3CUm",
     "timestamp": 1655169179306
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
